{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "name": "Community Detection Clustering in Complex Networks using Gumbel Softmax.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Q1X6L7lOjBz"
      },
      "source": [
        "## Community Detection Clustering in Complex Networks using Gumbel Softmax"
      ],
      "id": "5Q1X6L7lOjBz"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oIhfBE6kOjB3"
      },
      "source": [
        "### Authors: Visaj Nirav Shah (201801016) and Vyom Saraf (201801062)\n",
        "#### Mentor: Prof. Mukesh Tiwari"
      ],
      "id": "oIhfBE6kOjB3"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o9riCN6AOjB4"
      },
      "source": [
        "*Parts of this code were adopted from the codebase of D. B. Acharya and H. Zhang, “Community detection clustering via gumbel softmax,” SN Computer Science, vol. 1, no. 5, aug 2020. [Online]. Available: https://doi.org/10.1007%2Fs42979-020-00264-2*"
      ],
      "id": "o9riCN6AOjB4"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rOOjGLBeOvrh"
      },
      "source": [
        "Note:- In case the cells are hidden, please click on the arrows on the left hand-side to expand that particular section."
      ],
      "id": "rOOjGLBeOvrh"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p5VmjNbtOjCF"
      },
      "source": [
        "**Structure of the Notebook**"
      ],
      "id": "p5VmjNbtOjCF"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oKk6-lHLOjCG"
      },
      "source": [
        "- Section 1: Mathematics of Datasets\n",
        "- Section 2: Normal Clustering\n",
        "- Section 3: Custom Threshold Clustering (Multiple Clustering)\n",
        "- Section 4: Determining Ideal Number of Clusters\n",
        "- Section 5: Cluster Network Analysis"
      ],
      "id": "oKk6-lHLOjCG"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9QW-KWxTOjCG"
      },
      "source": [
        "**Importing the Required Libraries**"
      ],
      "id": "9QW-KWxTOjCG"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vK9rsxNYOjCG"
      },
      "source": [
        "import math\n",
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import matplotlib.animation as animation\n",
        "import networkx as nx\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from collections import deque\n",
        "from itertools import product\n",
        "from scipy import sparse\n",
        "from scipy.linalg import eig\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics.cluster import normalized_mutual_info_score,adjusted_rand_score\n",
        "\n",
        "device = torch.device('cpu')"
      ],
      "id": "vK9rsxNYOjCG",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-5F8VNRzOjCI"
      },
      "source": [
        "# Section 1: Mathematics of Datasets"
      ],
      "id": "-5F8VNRzOjCI"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9GkuszcnOjCI"
      },
      "source": [
        "### Zachary's Karate Club Dataset"
      ],
      "id": "9GkuszcnOjCI"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "St1BzcJdOjCJ"
      },
      "source": [
        "# Creating a Networkx Graph\n",
        "\n",
        "G = nx.karate_club_graph()"
      ],
      "id": "St1BzcJdOjCJ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z8sq1CIXOjCJ"
      },
      "source": [
        "# Number of nodes and edges\n",
        "\n",
        "print(\"The network has \" + str(G.number_of_nodes()) + \" nodes.\")\n",
        "print(\"The network has \" + str(G.number_of_edges()) + \" edges.\")"
      ],
      "id": "z8sq1CIXOjCJ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-dCtXUImOjCK"
      },
      "source": [
        "# Adjacency Matrix\n",
        "\n",
        "adj_mat = nx.adjacency_matrix(G).toarray()\n",
        "adj_mat"
      ],
      "id": "-dCtXUImOjCK",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NA4lF5RiOjCK"
      },
      "source": [
        "# Degree of nodes\n",
        "\n",
        "degree = {}\n",
        "for i in range(adj_mat.shape[0]):\n",
        "    degree[i] = sum(adj_mat[i])\n",
        "degree"
      ],
      "id": "NA4lF5RiOjCK",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NI1iEGRvOjCK"
      },
      "source": [
        "# Mean Degree\n",
        "\n",
        "sumD = 0\n",
        "for i in degree.keys():\n",
        "    sumD += degree[i]\n",
        "meanD = sumD / adj_mat.shape[0]\n",
        "meanD"
      ],
      "id": "NI1iEGRvOjCK",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pqi3F-jeOjCL"
      },
      "source": [
        "# Density\n",
        "\n",
        "density = meanD / (adj_mat.shape[0] - 1)\n",
        "density"
      ],
      "id": "Pqi3F-jeOjCL",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eQmotRwDOjCL"
      },
      "source": [
        "# Degree Distribution plot\n",
        "\n",
        "degree_list = sorted(set(degree.values()))\n",
        "count_list = [list(degree.values()).count(i) for i in degree_list]\n",
        "\n",
        "plt.scatter(degree_list, count_list)\n",
        "plt.xscale('log')\n",
        "plt.yscale('log')\n",
        "plt.xlabel('Degree')\n",
        "plt.ylabel('Number of Nodes')\n",
        "plt.title('Logarithmic Scatter Plot - Degree Distribution')\n",
        "plt.savefig('Karate Degree Distribution')"
      ],
      "id": "eQmotRwDOjCL",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9U9eV_VeOjCM"
      },
      "source": [
        "# graph Laplacian\n",
        "\n",
        "graph_laplacian = nx.laplacian_matrix(G).toarray()\n",
        "graph_laplacian"
      ],
      "id": "9U9eV_VeOjCM",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YRcTD4yAOjCM"
      },
      "source": [
        "# Betweenness\n",
        "\n",
        "between = nx.betweenness_centrality(G)\n",
        "between"
      ],
      "id": "YRcTD4yAOjCM",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "68FonJyBOjCM"
      },
      "source": [
        "# Closeness\n",
        "\n",
        "close = nx.closeness_centrality(G)\n",
        "close"
      ],
      "id": "68FonJyBOjCM",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N3Bh7KspOjCN"
      },
      "source": [
        "# Eigenvector centrality\n",
        "\n",
        "eigenvector_centrality = nx.eigenvector_centrality(G)\n",
        "eigenvector_centrality"
      ],
      "id": "N3Bh7KspOjCN",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yrWff2V2OjCO"
      },
      "source": [
        "desc = {}\n",
        "desc['Quantity'] = ['Nodes', 'Edges', 'Mean Degree', 'Density']\n",
        "desc['Value'] = [G.number_of_nodes(), G.number_of_edges(), meanD, density]\n",
        "\n",
        "desc_df = pd.DataFrame(desc)\n",
        "desc_df"
      ],
      "id": "yrWff2V2OjCO",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wSGGpM1aOjCO"
      },
      "source": [
        "### Facebook Page-Page Networks Dataset of Artists’ Pages"
      ],
      "id": "wSGGpM1aOjCO"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-bomfhJtOjCO"
      },
      "source": [
        "# Creating a Networkx Graph\n",
        "\n",
        "network1 = pd.read_csv(r\"artist_edges.csv\")\n",
        "edge_list = []\n",
        "for i in range(819306):\n",
        "    edge_list.append((network1.iloc[i]['node_1'], network1.iloc[i]['node_2']))\n",
        "\n",
        "G1 = nx.Graph()\n",
        "G1.add_edges_from(edge_list)"
      ],
      "id": "-bomfhJtOjCO",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MRlBpDSrOjCP"
      },
      "source": [
        "# Number of nodes and edges\n",
        "\n",
        "print(\"The network has \" + str(G1.number_of_nodes()) + \" nodes.\")\n",
        "print(\"The network has \" + str(G1.number_of_edges()) + \" edges.\")"
      ],
      "id": "MRlBpDSrOjCP",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ILBSW6COOjCP"
      },
      "source": [
        "# Adjacency Matrix\n",
        "\n",
        "adj_mat1 = nx.adjacency_matrix(G1).toarray()\n",
        "adj_mat1"
      ],
      "id": "ILBSW6COOjCP",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ieo8JEvKOjCP"
      },
      "source": [
        "# Degree of nodes\n",
        "\n",
        "degree1 = {}\n",
        "for i in range(adj_mat1.shape[0]):\n",
        "    degree1[i] = sum(adj_mat1[i])"
      ],
      "id": "Ieo8JEvKOjCP",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qBPQ-zEOOjCQ"
      },
      "source": [
        "# Mean Degree\n",
        "\n",
        "sumD = 0\n",
        "for i in degree1.keys():\n",
        "    sumD += degree1[i]\n",
        "meanD1 = sumD / adj_mat1.shape[0]\n",
        "meanD1"
      ],
      "id": "qBPQ-zEOOjCQ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "41N4Qc00OjCQ"
      },
      "source": [
        "# Density\n",
        "\n",
        "density1 = meanD1 / (adj_mat1.shape[0] - 1)\n",
        "density1"
      ],
      "id": "41N4Qc00OjCQ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hb2HanTMOjCQ"
      },
      "source": [
        "# Degree Distribution plot\n",
        "\n",
        "degree_list1 = sorted(set(degree1.values()))\n",
        "count_list1 = [list(degree1.values()).count(i) for i in degree_list1]\n",
        "\n",
        "plt.scatter(degree_list1, count_list1)\n",
        "plt.xscale('log')\n",
        "plt.yscale('log')\n",
        "plt.xlabel('Degree')\n",
        "plt.ylabel('Number of Nodes')\n",
        "plt.title('Logarithmic Scatter Plot - Degree Distribution')\n",
        "plt.savefig('Facebook Degree Distribution')"
      ],
      "id": "hb2HanTMOjCQ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "26ty4sHFOjCR"
      },
      "source": [
        "# graph Laplacian\n",
        "\n",
        "graph_laplacian = nx.laplacian_matrix(G1).toarray()\n",
        "graph_laplacian"
      ],
      "id": "26ty4sHFOjCR",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ImzqNjznOjCR"
      },
      "source": [
        "# Betweenness\n",
        "\n",
        "between1 = nx.betweenness_centrality(G1)\n",
        "between1"
      ],
      "id": "ImzqNjznOjCR",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i8qgcr7gOjCR"
      },
      "source": [
        "# Closeness\n",
        "\n",
        "close1 = nx.closeness_centrality(G1)\n",
        "close1"
      ],
      "id": "i8qgcr7gOjCR",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PgizladmOjCR"
      },
      "source": [
        "# Eigenvector centrality\n",
        "\n",
        "eigenvector_centrality = nx.eigenvector_centrality(G1)\n",
        "eigenvector_centrality"
      ],
      "id": "PgizladmOjCR",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KAOoL8k0OjCR"
      },
      "source": [
        "desc1 = {}\n",
        "desc1['Quantity'] = ['Nodes', 'Edges', 'Mean Degree', 'Density']\n",
        "desc1['Value'] = [G1.number_of_nodes(), G1.number_of_edges(), meanD1, density1]\n",
        "\n",
        "desc1_df = pd.DataFrame(desc1)\n",
        "desc1_df"
      ],
      "id": "KAOoL8k0OjCR",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gJqA2F0zOjCR"
      },
      "source": [
        "# Since our personal computers cannot process complex computations on such large datasets,\n",
        "# we will select 2500 nodes with the highest degree.\n",
        "# Only edges between these 2500 nodes will be considered.\n",
        "# Some nodes will be lost out of these 2500 because \n",
        "# some of them will only have edges with the nodes not considered in 2500.\n",
        "\n",
        "sorted_keys = sorted(degree1, key = degree1.get, reverse = True)\n",
        "selected_nodes = []\n",
        "for i in range(2500):\n",
        "    selected_nodes.append(sorted_keys[i])\n",
        "\n",
        "new_edge_list_temp = []\n",
        "for i in edge_list:\n",
        "    if (i[0] in selected_nodes) and (i[1] in selected_nodes):\n",
        "        new_edge_list_temp.append((i[0], i[1]))\n",
        "        \n",
        "G_Facebook_temp = nx.Graph()\n",
        "G_Facebook_temp.add_edges_from(new_edge_list_temp)"
      ],
      "id": "gJqA2F0zOjCR",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H-5ZtCB6OjCS"
      },
      "source": [
        "G_Facebook_nodes = sorted(list(G_Facebook_temp.nodes))\n",
        "\n",
        "d = {}\n",
        "count = 0\n",
        "for i in G_Facebook_nodes:\n",
        "    d[i] = count\n",
        "    count += 1\n",
        "    \n",
        "new_edge_list = []\n",
        "for i in new_edge_list_temp:\n",
        "    new_edge_list.append((d[i[0]], d[i[1]]))\n",
        "    \n",
        "G_Facebook = nx.Graph()\n",
        "G_Facebook.add_edges_from(new_edge_list)"
      ],
      "id": "H-5ZtCB6OjCS",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WUe5uKkrOjCS"
      },
      "source": [
        "# Number of nodes and edges\n",
        "\n",
        "print(\"The network has \" + str(G_Facebook.number_of_nodes()) + \" nodes.\")\n",
        "print(\"The network has \" + str(G_Facebook.number_of_edges()) + \" edges.\")"
      ],
      "id": "WUe5uKkrOjCS",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qdX_p8TmOjCS"
      },
      "source": [
        "# Functions"
      ],
      "id": "qdX_p8TmOjCS"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hwjen2V1OjCT"
      },
      "source": [
        "def get_base_modularity_matrix(network):\n",
        "    '''\n",
        "    Obtain the modularity matrix for the whole network\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    network : nx.Graph or nx.DiGraph\n",
        "        The network of interest\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    np.matrix\n",
        "        The modularity matrix for `network`\n",
        "\n",
        "    Raises\n",
        "    ------\n",
        "    TypeError\n",
        "        When the input `network` does not fit either nx.Graph or nx.DiGraph\n",
        "    '''\n",
        "\n",
        "    if type(network) == nx.Graph:\n",
        "        return sparse.csc_matrix(nx.modularity_matrix(network))\n",
        "    elif type(network) == nx.DiGraph:\n",
        "        return sparse.csc_matrix(nx.directed_modularity_matrix(network))\n",
        "    else:\n",
        "        raise TypeError('Graph type not supported. Use either nx.Graph or nx.Digraph')\n",
        "\n",
        "def _get_delta_Q(X, a):\n",
        "    '''\n",
        "    Calculate the detal modularity\n",
        "    .. math::\n",
        "        \\deltaQ = s^T \\cdot \\^{B_{g}} \\cdot s\n",
        "    .. math:: \\deltaQ = s^T \\cdot \\^{B_{g}} \\cdot s\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    X : np.matrix\n",
        "        B_hat_g\n",
        "    a : np.matrix\n",
        "        s, which is the membership vector\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    float\n",
        "        The corresponding :math:`\\deltaQ`\n",
        "    '''\n",
        "\n",
        "    delta_Q = (a.T.dot(X)).dot(a)\n",
        "\n",
        "    return delta_Q[0,0]\n",
        "\n",
        "def get_modularity(network, community_dict):\n",
        "    '''\n",
        "    Calculate the modularity. Edge weights are ignored.\n",
        "\n",
        "    Undirected:\n",
        "    .. math:: Q = \\frac{1}{2m}\\sum_{i,j} \\(A_ij - \\frac{k_i k_j}{2m}\\) * \\detal_(c_i, c_j)\n",
        "\n",
        "    Directed:\n",
        "    .. math:: Q = \\frac{1}{m}\\sum_{i,j} \\(A_ij - \\frac{k_i^{in} k_j^{out}}{m}\\) * \\detal_{c_i, c_j}\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    network : nx.Graph or nx.DiGraph\n",
        "        The network of interest\n",
        "    community_dict : dict\n",
        "        A dictionary to store the membership of each node\n",
        "        Key is node and value is community index\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    float\n",
        "        The modularity of `network` given `community_dict`\n",
        "    '''\n",
        "\n",
        "    Q = 0\n",
        "    G = network.copy()\n",
        "    nx.set_edge_attributes(G, {e:1 for e in G.edges}, 'weight')\n",
        "    A = nx.to_scipy_sparse_matrix(G).astype(float)\n",
        "\n",
        "    if type(G) == nx.Graph:\n",
        "        # for undirected graphs, in and out treated as the same thing\n",
        "        out_degree = in_degree = dict(nx.degree(G))\n",
        "        M = 2.*(G.number_of_edges())\n",
        "        print(\"Calculating modularity for undirected graph\")\n",
        "    elif type(G) == nx.DiGraph:\n",
        "        in_degree = dict(G.in_degree())\n",
        "        out_degree = dict(G.out_degree())\n",
        "        M = 1.*G.number_of_edges()\n",
        "        print(\"Calculating modularity for directed graph\")\n",
        "    else:\n",
        "        print('Invalid graph type')\n",
        "        raise TypeError\n",
        "\n",
        "    nodes = list(G)\n",
        "    Q = np.sum([A[i,j] - in_degree[nodes[i]]*\\\n",
        "                         out_degree[nodes[j]]/M\\\n",
        "                 for i, j in product(range(len(nodes)),\\\n",
        "                                     range(len(nodes))) \\\n",
        "                if community_dict[nodes[i]] == community_dict[nodes[j]]])\n",
        "    return Q / M\n",
        "\n",
        "def get_mod_matrix(network, comm_nodes=None, B=None):\n",
        "    '''\n",
        "    This function computes the modularity matrix\n",
        "    for a specific group in the network.\n",
        "    (a.k.a., generalized modularity matrix)\n",
        "\n",
        "    Specifically,\n",
        "    .. math::\n",
        "        B^g_{i,j} = B_ij - \\delta_{ij} \\sum_(k \\in g) B_ik\n",
        "        m = \\abs[\\Big]{E}\n",
        "        B_ij = A_ij - \\dfrac{k_i k_j}{2m}\n",
        "        OR...\n",
        "        B_ij = \\(A_ij - \\frac{k_i^{in} k_j^{out}}{m}\n",
        "\n",
        "    When `comm_nodes` is None or all nodes in `network`, this reduces to :math:`B`\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    network : nx.Graph or nx.DiGraph\n",
        "        The network of interest\n",
        "    comm_nodes : iterable (list, np.array, or tuple)\n",
        "        List of nodes that defines a community\n",
        "    B : np.matrix\n",
        "        Modularity matrix of `network`\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    np.matrix\n",
        "        The modularity of `comm_nodes` within `network`\n",
        "    '''\n",
        "\n",
        "    if comm_nodes is None:\n",
        "        comm_nodes = list(network)\n",
        "        return get_base_modularity_matrix(network)\n",
        "\n",
        "    if B is None:\n",
        "        B = get_base_modularity_matrix(network)\n",
        "\n",
        "    # subset of mod matrix in g\n",
        "    indices = [list(network).index(u) for u in comm_nodes]\n",
        "    B_g = B[indices, :][:, indices]\n",
        "    #print 'Type of `B_g`:', type(B_g)\n",
        "\n",
        "    # B^g_(i,j) = B_ij - δ_ij * ∑_(k∈g) B_ik\n",
        "    # i, j ∈ g\n",
        "    B_hat_g = np.zeros((len(comm_nodes), len(comm_nodes)), dtype=float)\n",
        "\n",
        "    # ∑_(k∈g) B_ik\n",
        "    B_g_rowsum = np.asarray(B_g.sum(axis=1))[:, 0]\n",
        "    if type(network) == nx.Graph:\n",
        "        B_g_colsum = np.copy(B_g_rowsum)\n",
        "    elif type(network) == nx.DiGraph:\n",
        "        B_g_colsum = np.asarray(B_g.sum(axis=0))[0, :]\n",
        "\n",
        "    for i in range(B_hat_g.shape[0]):\n",
        "        for j in range(B_hat_g.shape[0]):\n",
        "            if i == j:\n",
        "                B_hat_g[i,j] = B_g[i,j] - 0.5 * (B_g_rowsum[i] + B_g_colsum[i])\n",
        "            else:\n",
        "                B_hat_g[i,j] = B_g[i,j]\n",
        "\n",
        "    if type(network) == nx.DiGraph:\n",
        "        B_hat_g = B_hat_g + B_hat_g.T\n",
        "\n",
        "    return sparse.csc_matrix(B_hat_g)\n",
        "\n",
        "def largest_eig(A):\n",
        "    '''\n",
        "        A wrapper over `scipy.linalg.eig` to produce\n",
        "        largest eigval and eigvector for A when A.shape is small\n",
        "    '''\n",
        "    vals, vectors = eig(A.todense())\n",
        "    real_indices = [idx for idx, val in enumerate(vals) if not bool(val.imag)]\n",
        "    vals = [vals[i].real for i in range(len(real_indices))]\n",
        "    vectors = [vectors[i] for i in range(len(real_indices))]\n",
        "    max_idx = np.argsort(vals)[-1]\n",
        "    return np.asarray([vals[max_idx]]), np.asarray([vectors[max_idx]]).T"
      ],
      "id": "Hwjen2V1OjCT",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9_2GkUJKOjCU"
      },
      "source": [
        "def _divide(network, community_dict, comm_index, B, refine=False):\n",
        "    '''\n",
        "    Bisection of a community in `network`.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    network : nx.Graph or nx.DiGraph\n",
        "        The network of interest\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    tuple\n",
        "        If the given community is indivisible, return (None, None)\n",
        "        If the given community is divisible, return a tuple where\n",
        "        the 1st element is a node list for the 1st sub-group and\n",
        "        the 2nd element is a node list for the original group\n",
        "    '''\n",
        "\n",
        "    comm_nodes = tuple(u for u in community_dict \\\n",
        "                  if community_dict[u] == comm_index)\n",
        "    B_hat_g = get_mod_matrix(network, comm_nodes, B)\n",
        "\n",
        "    # compute the top eigenvector u₁ and β₁\n",
        "    if B_hat_g.shape[0] < 3:\n",
        "        beta_s, u_s = largest_eig(B_hat_g)\n",
        "    else:\n",
        "        beta_s, u_s = sparse.linalg.eigs(B_hat_g, k=1, which='LR')\n",
        "    u_1 = u_s[:, 0]\n",
        "    beta_1 = beta_s[0]\n",
        "    if beta_1 > 0:\n",
        "        # divisible\n",
        "        s = sparse.csc_matrix(np.asmatrix([[1 if u_1_i > 0 else -1] for u_1_i in u_1]))\n",
        "        if refine:\n",
        "            improve_modularity(network, comm_nodes, s, B)\n",
        "        delta_modularity = _get_delta_Q(B_hat_g, s)\n",
        "        if delta_modularity > 0:\n",
        "            g1_nodes = np.array([comm_nodes[i] \\\n",
        "                                 for i in range(u_1.shape[0]) \\\n",
        "                                 if s[i,0] > 0])\n",
        "            #g1 = nx.subgraph(g, g1_nodes)\n",
        "            if len(g1_nodes) == len(comm_nodes) or len(g1_nodes) == 0:\n",
        "                # indivisble, return None\n",
        "                return None, None\n",
        "            # divisible, return node list for one of the groups\n",
        "            return g1_nodes, comm_nodes\n",
        "    # indivisble, return None\n",
        "    return None, None\n",
        "\n",
        "def improve_modularity(network, comm_nodes, s, B):\n",
        "    '''\n",
        "    Fine tuning of the initial division from `_divide`\n",
        "    Modify `s` inplace\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    network : nx.Graph or nx.DiGraph\n",
        "        The network of interest\n",
        "    comm_nodes: iterable\n",
        "        List of nodes for the original group\n",
        "    s: np.matrix\n",
        "        A matrix of node membership. Only +1/-1\n",
        "    B: np.amtrix\n",
        "        Modularity matrix for `network`\n",
        "    '''\n",
        "\n",
        "    # iterate until no increment of Q\n",
        "    B_hat_g = get_mod_matrix(network, comm_nodes, B)\n",
        "    while True:\n",
        "        unmoved = list(comm_nodes)\n",
        "        # node indices to be moved\n",
        "        node_indices = np.array([], dtype=int)\n",
        "        # cumulative improvement after moving\n",
        "        node_improvement = np.array([], dtype=float)\n",
        "        # keep moving until none left\n",
        "        while len(unmoved) > 0:\n",
        "            # init Q\n",
        "            Q0 = _get_delta_Q(B_hat_g, s)\n",
        "            scores = np.zeros(len(unmoved))\n",
        "            for k_index in range(scores.size):\n",
        "                k = comm_nodes.index(unmoved[k_index])\n",
        "                s[k, 0] = -s[k, 0]\n",
        "                scores[k_index] = _get_delta_Q(B_hat_g, s) - Q0\n",
        "                s[k, 0] = -s[k, 0]\n",
        "            _j = np.argmax(scores)\n",
        "            j = comm_nodes.index(unmoved[_j])\n",
        "            # move j, which has the largest increase or smallest decrease\n",
        "            s[j, 0] = -s[j, 0]\n",
        "            node_indices = np.append(node_indices, j)\n",
        "            if node_improvement.size < 1:\n",
        "                node_improvement = np.append(node_improvement, scores[_j])\n",
        "            else:\n",
        "                node_improvement = np.append(node_improvement, \\\n",
        "                                        node_improvement[-1]+scores[_j])\n",
        "            #print len(unmoved), 'max: ', max(scores), node_improvement[-1]\n",
        "            unmoved.pop(_j)\n",
        "        # the biggest improvement\n",
        "        max_index = np.argmax(node_improvement)\n",
        "        # change all the remaining nodes\n",
        "        # which are not helping\n",
        "        for i in range(max_index+1, len(comm_nodes)):\n",
        "            j = node_indices[i]\n",
        "            s[j,0] = -s[j, 0]\n",
        "        # if we swap all the nodes, it is actually doing nothing\n",
        "        if max_index == len(comm_nodes) - 1:\n",
        "            delta_modularity = 0\n",
        "        else:\n",
        "            delta_modularity = node_improvement[max_index]\n",
        "        # Stop if ΔQ <= 0 \n",
        "        if delta_modularity <= 0:\n",
        "            break"
      ],
      "id": "9_2GkUJKOjCU",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zyBOYqXKOjCV"
      },
      "source": [
        "class CDCGS(nn.Module):\n",
        "  def __init__(self, A_hat, num_feat, num_hidden):\n",
        "    super(CDCGS, self).__init__()\n",
        "    self.num_feat = num_feat\n",
        "    self.num_hidden = num_hidden\n",
        "    self.A_hat = A_hat\n",
        "    self.W_0 = nn.Parameter(torch.ones(num_feat, num_hidden))\n",
        "    I = F.one_hot(torch.tensor(random.sample(range(0, num_feat), num_feat)), num_classes=num_feat)\n",
        "    self.I = torch.tensor(I,dtype=torch.float)\n",
        "\n",
        "  def forward(self, X, A_hat,temp):\n",
        "    global featureSelector\n",
        "    global weight_feature\n",
        "    featureSelector = self.W_0\n",
        "    results = torch.zeros(self.W_0.size())\n",
        "    x = 500\n",
        "    for i in range(x):\n",
        "        results += F.gumbel_softmax(self.W_0,tau=temp,hard=False)\n",
        "    weight_feature = results/x\n",
        "    \n",
        "    H = torch.mm(torch.mm(self.I,A_hat),self.I.T)\n",
        "    H = torch.mm(torch.mm(weight_feature.T,A_hat),weight_feature)\n",
        "    H = torch.div(H, H.sum(axis=0))\n",
        "    m = nn.Softmax(dim=0)\n",
        "    return m(H)"
      ],
      "id": "zyBOYqXKOjCV",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w5VNcxUxOjCV"
      },
      "source": [
        "def clustering(G, clusters_number):\n",
        "    A_hat = nx.adjacency_matrix(G).todense()\n",
        "    X = np.identity(G.number_of_nodes(), dtype=np.float)\n",
        "\n",
        "    num_feat = len(G.nodes())\n",
        "    num_hidden = clusters_number\n",
        "\n",
        "    model = CDCGS(A_hat, num_feat, num_hidden).to(device)\n",
        "\n",
        "    def lossFn(output,exp): \n",
        "        return torch.sum((torch.diag(-torch.log(output))))\n",
        "\n",
        "    optimizer = optim.Adam(model.parameters(),lr=1e-2,weight_decay=0,betas=(0.5, 0.999), eps=1e-08)\n",
        "\n",
        "    A_hat_tensor = torch.Tensor(A_hat).to(device)\n",
        "    X_tensor = torch.Tensor(X).to(device)\n",
        "\n",
        "    loss_hist = []\n",
        "    acc_hist = []\n",
        "    temp = 4\n",
        "    logits=[]\n",
        "    for epoch in range(600):\n",
        "        model.train()\n",
        "        model.zero_grad()\n",
        "        if(epoch == 75):\n",
        "            temp = 2.5\n",
        "        elif(epoch == 100):\n",
        "            temp = 2\n",
        "        elif(epoch == 150):\n",
        "            temp = 1.5\n",
        "        elif(epoch == 200):\n",
        "            temp = 1\n",
        "        elif(epoch == 300):\n",
        "            temp = 0.5\n",
        "        elif(epoch == 400):\n",
        "            temp = 0.25\n",
        "        elif(epoch == 500):\n",
        "            temp = 0.1\n",
        "        output = model(X_tensor, A_hat_tensor,temp)\n",
        "        loss = lossFn(output,torch.diag(torch.ones(output.size()[0])))\n",
        "        loss_hist.append(loss.item())\n",
        "        loss.backward()\n",
        "        optimizer.step()"
      ],
      "id": "w5VNcxUxOjCV",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kEhZupz-OjCW"
      },
      "source": [
        "def func1(gumbel_matrix, G):\n",
        "    gumbel_matrix_list = gumbel_matrix.tolist()\n",
        "    d = {}\n",
        "    d['node_num'] = []\n",
        "    d['cluster_num'] = []\n",
        "    for i in range(G.number_of_nodes()):\n",
        "        d['node_num'].append(i)\n",
        "        d['cluster_num'].append(gumbel_matrix_list[i])\n",
        "\n",
        "    df = pd.DataFrame(d)\n",
        "    return df"
      ],
      "id": "kEhZupz-OjCW",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9_StDSZIOjCW"
      },
      "source": [
        "def func2(gumbel_matrix, G):\n",
        "    d = {}\n",
        "    d['node_num'] = []\n",
        "    d['cluster_num'] = []\n",
        "    for i in range(G.number_of_nodes()):\n",
        "        d['node_num'].append(i)\n",
        "        d['cluster_num'].append(gumbel_matrix[i])\n",
        "        \n",
        "    df = pd.DataFrame(d)\n",
        "    return df"
      ],
      "id": "9_StDSZIOjCW",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-1934mYdOjCW"
      },
      "source": [
        "def cluster_network_analysis(G, gumbel_matrix, df):\n",
        "    \n",
        "    temp_D = {}\n",
        "    temp_D['Threshold Value'] = []\n",
        "    temp_D['Edges Left in the Network'] = []\n",
        "    a = set()\n",
        "    number_of_nodes = G.number_of_nodes()  \n",
        "    for i in range(number_of_nodes):\n",
        "        a.add(gumbel_matrix[i])\n",
        "\n",
        "    b = len(a)\n",
        "    cluster_matrix = [[0 for i in range(b)] for j in range(b)]\n",
        "    \n",
        "    for i in range(G.number_of_edges()):\n",
        "        if gumbel_matrix[df.iloc[i][0] - 1] != gumbel_matrix[df.iloc[i][1] - 1]:\n",
        "            cluster_matrix[gumbel_matrix[df.iloc[i][0] - 1]][gumbel_matrix[df.iloc[i][1] - 1]] = \\\n",
        "            cluster_matrix[gumbel_matrix[df.iloc[i][0] - 1]][gumbel_matrix[df.iloc[i][1] - 1]] + 1\n",
        "            cluster_matrix[gumbel_matrix[df.iloc[i][1] - 1]][gumbel_matrix[df.iloc[i][0] - 1]] = \\\n",
        "            cluster_matrix[gumbel_matrix[df.iloc[i][1] - 1]][gumbel_matrix[df.iloc[i][0] - 1]] + 1\n",
        "\n",
        "    maximum_connections = max([max(v) for v in cluster_matrix]) \n",
        "    print('The maximum number of connections between any two clusters is : ')\n",
        "    print(maximum_connections)\n",
        "    print('')\n",
        "    final_matrix = [[0 for i in range(b)] for j in range(b)]\n",
        "    for i in range (b):\n",
        "        for j in range (b):\n",
        "            final_matrix[i][j] = cluster_matrix[i][j] / maximum_connections\n",
        "\n",
        "    number_of_connections_left = 0\n",
        "\n",
        "    temp10 = 0\n",
        "    variable_matrix10 = final_matrix\n",
        "    for i in range (b):\n",
        "        for j in range (b):\n",
        "            if variable_matrix10[i][j] < 0 :\n",
        "                variable_matrix10[i][j] = 0\n",
        "\n",
        "    for i in range (b):\n",
        "        for j in range (b):\n",
        "            if variable_matrix10[i][j] != 0:\n",
        "                #temp10 = temp10 + (cluster_matrix[i][j]*maximum_connections)  \n",
        "                temp10 = temp10 + cluster_matrix[i][j]\n",
        "\n",
        "    temp10 = temp10/2             \n",
        "\n",
        "    '''for i in range (b):\n",
        "        for j in range (b):\n",
        "            variable_matrix10[i][j] = variable_matrix10[i][j]*maximum_connections'''\n",
        "    \n",
        "    #print(\"the adjacency matrix of the clusters assuming that the threshold value is 0 : \")\n",
        "    #print(variable_matrix10)\n",
        "    #print('')\n",
        "    #print('Number of edges left in the network for threshold value 0 is: ', temp10)\n",
        "    #print('')\n",
        "    #print('')\n",
        "    \n",
        "    temp_D['Threshold Value'].append(0.0)\n",
        "    temp_D['Edges Left in the Network'].append(temp10)\n",
        "    \n",
        "    temp6 = 0\n",
        "    variable_matrix6 = final_matrix\n",
        "    for i in range (b):\n",
        "        for j in range (b):\n",
        "            if variable_matrix6[i][j] < 0.1 :\n",
        "                variable_matrix6[i][j] = 0\n",
        "\n",
        "    for i in range (b):\n",
        "        for j in range (b):\n",
        "            if variable_matrix6[i][j] != 0:\n",
        "                #temp6 = temp6 + (cluster_matrix[i][j]*maximum_connections)  \n",
        "                temp6 = temp6 + cluster_matrix[i][j]\n",
        "\n",
        "    temp6 = temp6/2             \n",
        "\n",
        "    '''for i in range (b):\n",
        "        for j in range (b):\n",
        "            variable_matrix6[i][j] = variable_matrix6[i][j]*maximum_connections'''\n",
        "    \n",
        "    #print(\"the adjacency matrix of the clusters assuming that the threshold value is 0.1 : \")\n",
        "    #print(variable_matrix6)\n",
        "    #print('')\n",
        "    #print('Number of edges left in the network for threshold value 0.1 is: ', temp6)\n",
        "    #print('')\n",
        "    #print('')\n",
        "    \n",
        "    temp_D['Threshold Value'].append(0.1)\n",
        "    temp_D['Edges Left in the Network'].append(temp6)\n",
        "    \n",
        "    temp7 = 0\n",
        "    variable_matrix7 = final_matrix\n",
        "    for i in range (b):\n",
        "        for j in range (b):\n",
        "            if variable_matrix7[i][j] < 0.2 :\n",
        "                variable_matrix7[i][j] = 0\n",
        "\n",
        "    for i in range (b):\n",
        "        for j in range (b):\n",
        "            if variable_matrix7[i][j] != 0:\n",
        "                #temp7 = temp7 + (cluster_matrix[i][j]*maximum_connections)  \n",
        "                temp7 = temp7 + cluster_matrix[i][j]\n",
        "\n",
        "    temp7 = temp7/2             \n",
        "\n",
        "    '''for i in range (b):\n",
        "        for j in range (b):\n",
        "            variable_matrix7[i][j] = variable_matrix7[i][j]*maximum_connections'''\n",
        "    \n",
        "    #print(\"the adjacency matrix of the clusters assuming that the threshold value is 0.2 : \")\n",
        "    #print(variable_matrix7)\n",
        "    #print('')\n",
        "    #print('Number of edges left in the network for threshold value 0.2 is: ', temp7)\n",
        "    #print('')\n",
        "    #print('')\n",
        "    \n",
        "    temp_D['Threshold Value'].append(0.2)\n",
        "    temp_D['Edges Left in the Network'].append(temp7)\n",
        "    \n",
        "    temp4 = 0\n",
        "    variable_matrix5 = final_matrix\n",
        "    for i in range (b):\n",
        "        for j in range (b):\n",
        "            if variable_matrix5[i][j] < 0.3 :\n",
        "                variable_matrix5[i][j] = 0\n",
        "\n",
        "    for i in range (b):\n",
        "        for j in range (b):\n",
        "            if variable_matrix5[i][j] != 0:\n",
        "                #temp4 = temp4 + (cluster_matrix[i][j]*maximum_connections)  \n",
        "                temp4 = temp4 + cluster_matrix[i][j]\n",
        "\n",
        "    temp4 = temp4/2             \n",
        "\n",
        "    '''for i in range (b):\n",
        "        for j in range (b):\n",
        "            variable_matrix5[i][j] = variable_matrix5[i][j]*maximum_connections'''\n",
        "    \n",
        "    #print(\"the adjacency matrix of the clusters assuming that the threshold value is 0.3 : \")\n",
        "    #print(variable_matrix5)\n",
        "    #print('')\n",
        "    #print('Number of edges left in the network for threshold value 0.3 is: ', temp4)\n",
        "    #print('')\n",
        "    #print('')\n",
        "    \n",
        "    temp_D['Threshold Value'].append(0.3)\n",
        "    temp_D['Edges Left in the Network'].append(temp4)\n",
        "    \n",
        "    temp2  = 0\n",
        "\n",
        "    variable_matrix3 = final_matrix\n",
        "    for i in range (b):\n",
        "        for j in range (b):\n",
        "            if variable_matrix3[i][j] < 0.4 :\n",
        "                variable_matrix3[i][j] = 0\n",
        "\n",
        "    for i in range (b):\n",
        "        for j in range (b):\n",
        "            if variable_matrix3[i][j] != 0:\n",
        "                #temp2 = temp2 + (cluster_matrix[i][j]*maximum_connections)\n",
        "                temp2 = temp2 + cluster_matrix[i][j]\n",
        "\n",
        "    temp2 = temp2/2            \n",
        "\n",
        "    '''for i in range (b):\n",
        "        for j in range (b):\n",
        "            variable_matrix3[i][j] = variable_matrix3[i][j]*maximum_connections'''\n",
        "    \n",
        "    #print(\"the adjacency matrix of the clusters assuming that the threshold value is 0.4 : \")\n",
        "    #print(variable_matrix3)\n",
        "    #print('')\n",
        "    #print('Number of edges left in the network for threshold value 0.4 is: ', temp2)\n",
        "    #print('')\n",
        "    #print('')\n",
        "    \n",
        "    temp_D['Threshold Value'].append(0.4)\n",
        "    temp_D['Edges Left in the Network'].append(temp2)\n",
        "    \n",
        "    variable_matrix1 = final_matrix\n",
        "    for i in range (b):\n",
        "        for j in range (b):\n",
        "            if variable_matrix1[i][j] < 0.5 :\n",
        "                variable_matrix1[i][j] = 0\n",
        "\n",
        "    for i in range (b):\n",
        "        for j in range (b):\n",
        "            if variable_matrix1[i][j] != 0:\n",
        "                #number_of_connections_left = number_of_connections_left + (cluster_matrix[i][j]*maximum_connections)\n",
        "                number_of_connections_left = number_of_connections_left + cluster_matrix[i][j]\n",
        "\n",
        "    temp = 0.5\n",
        "    temp1 = 0\n",
        "    number_of_connections_left = number_of_connections_left/2\n",
        "\n",
        "    '''for i in range (b):\n",
        "        for j in range (b):\n",
        "            variable_matrix1[i][j] = variable_matrix1[i][j]*maximum_connections'''\n",
        "    \n",
        "    #print(\"the adjacency matrix of the clusters assuming that the threshold value is 0.5 : \")\n",
        "    #print(variable_matrix1)\n",
        "    #print('')\n",
        "    #print('Number of edges left in the network for threshold value 0.5 is: ', number_of_connections_left)\n",
        "    #print('')\n",
        "    #print('')\n",
        "    temp_D['Threshold Value'].append(0.5)\n",
        "    temp_D['Edges Left in the Network'].append(number_of_connections_left)\n",
        "\n",
        "    variable_matrix2 = final_matrix\n",
        "    for i in range (b):\n",
        "        for j in range (b):\n",
        "            if variable_matrix2[i][j] < 0.6 :\n",
        "                variable_matrix2[i][j] = 0\n",
        "\n",
        "    for i in range (b):\n",
        "        for j in range (b):\n",
        "            if variable_matrix2[i][j] != 0:\n",
        "                #temp1 = temp1 + (cluster_matrix[i][j]*maximum_connections)  \n",
        "                temp1 = temp1 + cluster_matrix[i][j]\n",
        "\n",
        "    temp1 = temp1/2\n",
        "\n",
        "    '''for i in range (b):\n",
        "        for j in range (b):\n",
        "            variable_matrix2[i][j] = variable_matrix2[i][j]*maximum_connections'''\n",
        "    \n",
        "    #print(\"the adjacency matrix of the clusters assuming that the threshold value is 0.6 : \")\n",
        "    #print(variable_matrix2)\n",
        "    #print('')\n",
        "    #print('Number of edges left in the network for threshold value 0.6 is: ', temp1)\n",
        "    #print('')\n",
        "    #print('')\n",
        "    \n",
        "    temp_D['Threshold Value'].append(0.6)\n",
        "    temp_D['Edges Left in the Network'].append(temp1)\n",
        "\n",
        "    temp3 = 0\n",
        "    variable_matrix4 = final_matrix\n",
        "    for i in range (b):\n",
        "        for j in range (b):\n",
        "            if variable_matrix4[i][j] < 0.7 :\n",
        "                variable_matrix4[i][j] = 0\n",
        "\n",
        "    for i in range (b):\n",
        "        for j in range (b):\n",
        "            if variable_matrix4[i][j] != 0:\n",
        "                #temp3 = temp3 + (cluster_matrix[i][j]*maximum_connections) \n",
        "                temp3 = temp3 + cluster_matrix[i][j]\n",
        "\n",
        "    temp3 = temp3/2       \n",
        "\n",
        "    ''' for i in range (b):\n",
        "        for j in range (b):\n",
        "            variable_matrix4[i][j] = variable_matrix4[i][j]*maximum_connections'''\n",
        "    \n",
        "    #print(\"the adjacency matrix of the clusters assuming that the threshold value is 0.7 : \")\n",
        "    #print(variable_matrix4)\n",
        "    #print('')\n",
        "    #print('Number of edges left in the network for threshold value 0.7 is: ', temp3)\n",
        "    #print('')\n",
        "    #print('')\n",
        "    \n",
        "    temp_D['Threshold Value'].append(0.7)\n",
        "    temp_D['Edges Left in the Network'].append(temp3)\n",
        "\n",
        "    temp8 = 0\n",
        "    variable_matrix8 = final_matrix\n",
        "    for i in range (b):\n",
        "        for j in range (b):\n",
        "            if variable_matrix8[i][j] < 0.8 :\n",
        "                variable_matrix8[i][j] = 0\n",
        "\n",
        "    for i in range (b):\n",
        "        for j in range (b):\n",
        "            if variable_matrix8[i][j] != 0:\n",
        "                #temp8 = temp8 + (cluster_matrix[i][j]*maximum_connections) \n",
        "                temp8 = temp8 + cluster_matrix[i][j]\n",
        "\n",
        "    temp8 = temp8/2       \n",
        "\n",
        "    ''' for i in range (b):\n",
        "        for j in range (b):\n",
        "            variable_matrix8[i][j] = variable_matrix8[i][j]*maximum_connections'''\n",
        "    \n",
        "    #print(\"the adjacency matrix of the clusters assuming that the threshold value is 0.8 : \")\n",
        "    #print(variable_matrix8)\n",
        "    #print('')\n",
        "    #print('Number of edges left in the network for threshold value 0.8 is: ', temp8)\n",
        "    #print('')\n",
        "    #print('')\n",
        "    \n",
        "    temp_D['Threshold Value'].append(0.8)\n",
        "    temp_D['Edges Left in the Network'].append(temp8)\n",
        "\n",
        "    \n",
        "    temp9 = 0\n",
        "    variable_matrix9 = final_matrix\n",
        "    for i in range (b):\n",
        "        for j in range (b):\n",
        "            if variable_matrix9[i][j] < 0.9 :\n",
        "                variable_matrix9[i][j] = 0\n",
        "\n",
        "    for i in range (b):\n",
        "        for j in range (b):\n",
        "            if variable_matrix9[i][j] != 0:\n",
        "                #temp9 = temp9 + (cluster_matrix[i][j]*maximum_connections) \n",
        "                temp9 = temp9 + cluster_matrix[i][j]\n",
        "\n",
        "    temp9 = temp9/2       \n",
        "\n",
        "    ''' for i in range (b):\n",
        "        for j in range (b):\n",
        "            variable_matrix9[i][j] = variable_matrix9[i][j]*maximum_connections'''\n",
        "    \n",
        "    #print(\"the adjacency matrix of the clusters assuming that the threshold value is 0.9 : \")\n",
        "    #print(variable_matrix9)\n",
        "    #print('')\n",
        "    #print('Number of edges left in the network for threshold value 0.9 is: ', temp9)\n",
        "    #print('')\n",
        "    #print('')\n",
        "    \n",
        "    temp_D['Threshold Value'].append(0.9)\n",
        "    temp_D['Edges Left in the Network'].append(temp9)\n",
        "    \n",
        "    df = pd.DataFrame(temp_D)\n",
        "    return df"
      ],
      "id": "-1934mYdOjCW",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PDldOl1GOjCX"
      },
      "source": [
        "# Section 2: Normal Clustering"
      ],
      "id": "PDldOl1GOjCX"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZO0-7AJ4OjCX"
      },
      "source": [
        "### Zachary's Karate Club Dataset"
      ],
      "id": "ZO0-7AJ4OjCX"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mOotkaNIOjCY"
      },
      "source": [
        "clusters_number = 2"
      ],
      "id": "mOotkaNIOjCY",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IspOa2huOjCY"
      },
      "source": [
        "# Running the Clustering\n",
        "\n",
        "clustering(G, clusters_number)"
      ],
      "id": "IspOa2huOjCY",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E8iJ8YW6OjCY"
      },
      "source": [
        "gumbel_matrix_karate = weight_feature.detach().max(dim=1)[1]\n",
        "labels_pred = gumbel_matrix_karate.data.numpy()\n",
        "\n",
        "print('Modularity for clustering in ' + str(clusters_number) + ' clusters is ' + str(get_modularity(G, labels_pred)) + '.')"
      ],
      "id": "E8iJ8YW6OjCY",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u3Xqm4zYOjCY"
      },
      "source": [
        "# Creating a DataFrame to store data about nodes and the cluster they belong to\n",
        "# Clusters: 0, 1\n",
        "\n",
        "df = func1(gumbel_matrix_karate, G)\n",
        "df.head()"
      ],
      "id": "u3Xqm4zYOjCY",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X_1JpTeIOjCY"
      },
      "source": [
        "# Size of each cluster\n",
        "\n",
        "df.groupby('cluster_num').size()"
      ],
      "id": "X_1JpTeIOjCY",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dSCAq5OUOjCY"
      },
      "source": [
        "### Facebook Dataset"
      ],
      "id": "dSCAq5OUOjCY"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z5bg1ozgOjCZ"
      },
      "source": [
        "clusters_number = 10"
      ],
      "id": "Z5bg1ozgOjCZ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LrqfQqkBOjCZ"
      },
      "source": [
        "# Running the Clustering\n",
        "\n",
        "clustering(G_Facebook, clusters_number)"
      ],
      "id": "LrqfQqkBOjCZ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XExOX0CyOjCZ"
      },
      "source": [
        "gumbel_matrix_facebook = weight_feature.detach().max(dim=1)[1]\n",
        "labels_pred = gumbel_matrix_facebook.data.numpy()\n",
        "\n",
        "print('Modularity for clustering in ' + str(clusters_number) + ' clusters is ' + str(get_modularity(G_Facebook, labels_pred)) + '.')"
      ],
      "id": "XExOX0CyOjCZ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XLLrT27kOjCZ"
      },
      "source": [
        "# Creating a DataFrame to store data about nodes and the cluster they belong to\n",
        "# Clusters: 0, 1, 2, 3, 4, 5, 6, 7, 8, 9\n",
        "\n",
        "df = func1(gumbel_matrix_facebook, G_Facebook)\n",
        "df.head()"
      ],
      "id": "XLLrT27kOjCZ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u1SQOWxoOjCZ"
      },
      "source": [
        "# Size of each cluster\n",
        "\n",
        "df.groupby('cluster_num').size()"
      ],
      "id": "u1SQOWxoOjCZ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JzZl0osOOjCZ"
      },
      "source": [
        "# Section 3: Custom Threshold Clustering (Multiple Clustering)"
      ],
      "id": "JzZl0osOOjCZ"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KN3tB-BROjCa"
      },
      "source": [
        "- Each node has a certain probability of belonging to a cluster.\n",
        "- In Section 2, for whichever cluster this probability was maximum, the node was assigned to that cluster.\n",
        "- Here, we assign all the clusters to a node for which the probability is more than or equal to a given threshold (threshold given by user)."
      ],
      "id": "KN3tB-BROjCa"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ErwEEEHsOjCa"
      },
      "source": [
        "### Zachary's Karate Club Dataset"
      ],
      "id": "ErwEEEHsOjCa"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ei0bCQBCOjCa"
      },
      "source": [
        "# These values can be changed by the user\n",
        "\n",
        "clusters_number = 5\n",
        "prob_threshold = 0.1"
      ],
      "id": "ei0bCQBCOjCa",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5WNBkLRCOjCa"
      },
      "source": [
        "# Running the Clustering\n",
        "\n",
        "clustering(G, clusters_number)"
      ],
      "id": "5WNBkLRCOjCa",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rDyd8e8tOjCa"
      },
      "source": [
        "gumbel_matrix = []\n",
        "\n",
        "for i in weight_feature.tolist():\n",
        "    node_cluster = []\n",
        "    for j in range(clusters_number):\n",
        "        if i[j] >= prob_threshold:\n",
        "            node_cluster.append(j)\n",
        "    gumbel_matrix.append(node_cluster)"
      ],
      "id": "rDyd8e8tOjCa",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "MG68vjSIOjCa"
      },
      "source": [
        "# Creating a DataFrame to store data about nodes and the cluster they belong to\n",
        "# Clusters: 0, 1, 2, 3, 4\n",
        "    \n",
        "df = func2(gumbel_matrix, G)\n",
        "df.head()"
      ],
      "id": "MG68vjSIOjCa",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4hzHfWeGOjCb"
      },
      "source": [
        "# Count of nodes assigned to more than 1 cluster\n",
        "\n",
        "count = 0\n",
        "for i in df['cluster_num']:\n",
        "    if len(i) > 1:\n",
        "        count += 1\n",
        "        \n",
        "print(str(count) + \" nodes got assigned to more than 1 cluster.\")"
      ],
      "id": "4hzHfWeGOjCb",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NZpHQf55OjCb"
      },
      "source": [
        "### Facebook Dataset"
      ],
      "id": "NZpHQf55OjCb"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9LW2_vOBOjCb"
      },
      "source": [
        "# These values can be changed by the user\n",
        "\n",
        "clusters_number = 50\n",
        "prob_threshold = 0.1"
      ],
      "id": "9LW2_vOBOjCb",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "80KcbOAEOjCb"
      },
      "source": [
        "# Running the Clustering\n",
        "\n",
        "clustering(G_Facebook, clusters_number)"
      ],
      "id": "80KcbOAEOjCb",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7vM-DOiiOjCb"
      },
      "source": [
        "gumbel_matrix = []\n",
        "\n",
        "for i in weight_feature.tolist():\n",
        "    node_cluster = []\n",
        "    for j in range(clusters_number):\n",
        "        if i[j] >= prob_threshold:\n",
        "            node_cluster.append(j)\n",
        "    gumbel_matrix.append(node_cluster)"
      ],
      "id": "7vM-DOiiOjCb",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Nw_AM8JOjCb"
      },
      "source": [
        "# Creating a DataFrame to store data about nodes and the cluster they belong to\n",
        "\n",
        "df = func2(gumbel_matrix, G_Facebook)\n",
        "df.head()"
      ],
      "id": "6Nw_AM8JOjCb",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cgmo4lKLOjCb"
      },
      "source": [
        "# Count of nodes assigned to more than 1 cluster\n",
        "\n",
        "count = 0\n",
        "for i in df['cluster_num']:\n",
        "    if len(i) > 1:\n",
        "        count += 1\n",
        "\n",
        "print(str(count) + \" nodes got assigned to more than 1 cluster.\")"
      ],
      "id": "cgmo4lKLOjCb",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iwHXGQdROjCc"
      },
      "source": [
        "# Section 4: Determining Ideal Number of Clusters"
      ],
      "id": "iwHXGQdROjCc"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "142YY3DtOjCc"
      },
      "source": [
        "- We plot a Modularity vs. Number of Clusters (clusters_number) graph for a fixed range of number of clusters we want.\n",
        "- The value of clusters_number for which the modularity is maximum is the ideal number of clusters."
      ],
      "id": "142YY3DtOjCc"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n3qSAwfKOjCc"
      },
      "source": [
        "### Zachary's Karate Club Dataset"
      ],
      "id": "n3qSAwfKOjCc"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_2KCCyGFOjCc"
      },
      "source": [
        "# Maintaining a list to save the modularity values\n",
        "\n",
        "modularity_list = []"
      ],
      "id": "_2KCCyGFOjCc",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "anniPnl-OjCc"
      },
      "source": [
        "# Run a loop for different number of clusters.\n",
        "# Range of clusters_number: [2, 11)\n",
        "\n",
        "for i in range(2, 11):\n",
        "   clustering(G, i)\n",
        "\n",
        "   gumbel_matrix = weight_feature.detach().max(dim=1)[1]\n",
        "   labels_pred = gumbel_matrix.data.numpy()\n",
        "   modularity_list.append((i, get_modularity(G, labels_pred)))"
      ],
      "id": "anniPnl-OjCc",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sd2oK-h3OjCc"
      },
      "source": [
        "xAxis = []\n",
        "yAxis = []\n",
        "d = {}\n",
        "for i in modularity_list:\n",
        "    xAxis.append(i[0])\n",
        "    yAxis.append(i[1])\n",
        "    d[i[1]] = i[0]\n",
        "    \n",
        "plt.plot(np.array(xAxis), np.array(yAxis))"
      ],
      "id": "Sd2oK-h3OjCc",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RoZP4OW0OjCc"
      },
      "source": [
        "print(\"Maximum modularity in this range = \", max(yAxis))\n",
        "print(\"Which is achieved for clusters_number = \", d[max(yAxis)])"
      ],
      "id": "RoZP4OW0OjCc",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xQrBpM2MOjCd"
      },
      "source": [
        "### Facebook Dataset"
      ],
      "id": "xQrBpM2MOjCd"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AGjz3AHpOjCd"
      },
      "source": [
        "# Maintaining a list to save the modularity values\n",
        "\n",
        "modularity_list = []"
      ],
      "id": "AGjz3AHpOjCd",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wmzonVv0OjCd"
      },
      "source": [
        "# Run a loop for different number of clusters.\n",
        "# Range of clusters_number: [10, 101)\n",
        "\n",
        "for i in range(10, 101):\n",
        "    clustering(G_Facebook, i)\n",
        "\n",
        "    gumbel_matrix = weight_feature.detach().max(dim=1)[1]\n",
        "    labels_pred = gumbel_matrix.data.numpy()\n",
        "    modularity_list.append((i, get_modularity(G_Facebook, labels_pred)))"
      ],
      "id": "wmzonVv0OjCd",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ogHjYnJbOjCd"
      },
      "source": [
        "xAxis = []\n",
        "yAxis = []\n",
        "d = {}\n",
        "for i in modularity_list:\n",
        "    xAxis.append(i[0])\n",
        "    yAxis.append(i[1])\n",
        "    d[i[1]] = i[0]\n",
        "    \n",
        "plt.plot(np.array(xAxis), np.array(yAxis))"
      ],
      "id": "ogHjYnJbOjCd",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "670-bKQSOjCd"
      },
      "source": [
        "print(\"Maximum modularity in this range = \", max(yAxis))\n",
        "print(\"Which is achieved for clusters_number = \", d[max(yAxis)])"
      ],
      "id": "670-bKQSOjCd",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qatTSGDXOjCd"
      },
      "source": [
        "# Section 5: Cluster Network Analysis"
      ],
      "id": "qatTSGDXOjCd"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4kBEWR4_OjCd"
      },
      "source": [
        "### Zachary's Karate Club Dataset"
      ],
      "id": "4kBEWR4_OjCd"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sJ248eAWOjCe"
      },
      "source": [
        "network0 = pd.read_csv(r\"karate_club.csv\")\n",
        "edge_list = []\n",
        "for i in range(78):\n",
        "    edge_list.append((network0.iloc[i]['node_1'], network0.iloc[i]['node_2']))\n",
        "    \n",
        "cluster_network_analysis(G, gumbel_matrix_karate.tolist(), network0)"
      ],
      "id": "sJ248eAWOjCe",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "weLOpwtCOjCe"
      },
      "source": [
        "### Facebook Dataset"
      ],
      "id": "weLOpwtCOjCe"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "njtlnoG7OjCe"
      },
      "source": [
        "d_edge = {}\n",
        "d_edge['node_1'] = []\n",
        "d_edge['node_2'] = []\n",
        "\n",
        "for i in new_edge_list:\n",
        "    d_edge['node_1'].append(i[0])\n",
        "    d_edge['node_2'].append(i[1])\n",
        "\n",
        "network1 = pd.DataFrame(d_edge)\n",
        "\n",
        "cluster_network_analysis(G_Facebook, gumbel_matrix_facebook, network1)"
      ],
      "id": "njtlnoG7OjCe",
      "execution_count": null,
      "outputs": []
    }
  ]
}